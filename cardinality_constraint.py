"""
BYRD Cardinality Constraint System
Global Cardinality Constraint with Costs (GCC-Costs) for memory graph regulation.

PHYSICS METAPHOR:
- Duplicate nodes are like bound states with high binding energy (costly)
- Orphan nodes are vacuum fluctuations not yet integrated into the lattice
- The "ground state" is achieved by minimizing total system energy:
  E_total = E_duplicate + E_orphan + E_connectivity_imbalance

This module implements constraints that:
1. Detect and penalize duplicate proliferation (high binding energy cost)
2. Facilitate orphan unification into the main lattice (lower vacuum energy)
3. Enforce cardinality bounds on node relationships
4. Track constraint violations as observable "energy" metrics

EMERGENCE PRINCIPLE:
The constraint system observes and reports; it does not prescribe what BYRD
should do. Violations are surfaced as experiences for BYRD to reflect upon.
"""

import asyncio
import math
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum

from event_bus import event_bus, Event, EventType


class ConstraintType(Enum):
    """Types of cardinality constraints."""
    MAX_DUPLICATES = "max_duplicates"           # Max similar nodes allowed
    MIN_CONNECTIVITY = "min_connectivity"       # Minimum connections per node
    MAX_CONNECTIVITY = "max_connectivity"       # Maximum connections per node
    ORPHAN_THRESHOLD = "orphan_threshold"       # Max orphans before consolidation
    BELIEF_CARDINALITY = "belief_cardinality"   # Max beliefs per topic cluster
    DESIRE_CARDINALITY = "desire_cardinality"   # Max active desires


class ViolationSeverity(Enum):
    """Severity levels for constraint violations."""
    LOW = "low"           # Minor deviation, informational
    MEDIUM = "medium"     # Noticeable imbalance, should address
    HIGH = "high"         # Significant violation, urgent
    CRITICAL = "critical" # System stability at risk


@dataclass
class ConstraintViolation:
    """
    A detected violation of a cardinality constraint.

    Violations are not errors - they are observations of system state
    that BYRD can reflect upon and potentially act on.
    """
    id: str
    constraint_type: ConstraintType
    severity: ViolationSeverity
    description: str

    # Quantitative measures
    current_value: float        # Current count/measure
    threshold_value: float      # Constraint threshold
    energy_cost: float          # "Energy" penalty for this violation

    # Affected nodes
    affected_node_ids: List[str]
    affected_node_type: str

    # Context
    detected_at: datetime
    context: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "constraint_type": self.constraint_type.value,
            "severity": self.severity.value,
            "description": self.description,
            "current_value": self.current_value,
            "threshold_value": self.threshold_value,
            "energy_cost": self.energy_cost,
            "affected_node_ids": self.affected_node_ids,
            "affected_node_type": self.affected_node_type,
            "detected_at": self.detected_at.isoformat(),
            "context": self.context
        }


@dataclass
class ConsolidationProposal:
    """
    A proposed action to reduce system energy by consolidating nodes.

    Proposals are generated by the constraint system but must be
    approved through BYRD's normal decision-making process.
    """
    id: str
    action_type: str            # merge, archive, link, prune
    source_node_ids: List[str]  # Nodes to consolidate from
    target_node_id: Optional[str]  # Node to consolidate into (for merge)

    # Energy analysis
    current_energy: float       # Energy before consolidation
    projected_energy: float     # Energy after consolidation
    energy_reduction: float     # Delta (should be positive for good proposals)

    # Justification
    reason: str
    confidence: float           # How confident we are this is a good action

    created_at: datetime

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "action_type": self.action_type,
            "source_node_ids": self.source_node_ids,
            "target_node_id": self.target_node_id,
            "current_energy": self.current_energy,
            "projected_energy": self.projected_energy,
            "energy_reduction": self.energy_reduction,
            "reason": self.reason,
            "confidence": self.confidence,
            "created_at": self.created_at.isoformat()
        }


@dataclass
class SystemEnergyState:
    """
    The current "energy state" of BYRD's memory graph.

    Lower energy = more organized, consolidated graph
    Higher energy = more disorder, duplicates, orphans
    """
    timestamp: datetime

    # Component energies
    duplicate_energy: float     # Cost of duplicate nodes
    orphan_energy: float        # Cost of unintegrated nodes
    connectivity_energy: float  # Cost of imbalanced connections
    fragmentation_energy: float # Cost of disconnected clusters

    # Aggregate
    total_energy: float
    ground_state_estimate: float  # Theoretical minimum energy
    excitation_level: float       # How far above ground state

    # Metrics
    duplicate_count: int
    orphan_count: int
    avg_connectivity: float
    cluster_count: int

    def to_dict(self) -> Dict[str, Any]:
        return {
            "timestamp": self.timestamp.isoformat(),
            "duplicate_energy": self.duplicate_energy,
            "orphan_energy": self.orphan_energy,
            "connectivity_energy": self.connectivity_energy,
            "fragmentation_energy": self.fragmentation_energy,
            "total_energy": self.total_energy,
            "ground_state_estimate": self.ground_state_estimate,
            "excitation_level": self.excitation_level,
            "duplicate_count": self.duplicate_count,
            "orphan_count": self.orphan_count,
            "avg_connectivity": self.avg_connectivity,
            "cluster_count": self.cluster_count
        }


class GlobalCardinalityConstraint:
    """
    Implements Global Cardinality Constraints with Costs for BYRD's memory graph.

    The GCC enforces:
    1. Upper bounds on similar/duplicate nodes (costly binding energy)
    2. Lower bounds on connectivity (orphan prevention)
    3. Balanced distribution of relationships (lattice regularity)
    4. Cluster size limits (prevent information bottlenecks)

    All constraints have associated "costs" (energy) that are tracked
    to guide the system toward its ground state.
    """

    # Default constraint configuration
    DEFAULT_CONSTRAINTS = {
        "max_belief_duplicates": {
            "threshold": 3,           # Max similar beliefs allowed
            "similarity_threshold": 0.85,
            "energy_per_violation": 1.0,
        },
        "max_orphan_ratio": {
            "threshold": 0.15,        # Max 15% orphan nodes
            "energy_per_percent": 0.5,
        },
        "min_node_connectivity": {
            "threshold": 1,           # Each node should have >= 1 connection
            "energy_per_isolated": 0.3,
        },
        "max_node_connectivity": {
            "threshold": 20,          # Prevent super-hub nodes
            "energy_per_excess": 0.1,
        },
        "max_active_desires": {
            "threshold": 10,          # Prevent desire proliferation
            "energy_per_excess": 0.5,
        },
        "max_cluster_imbalance": {
            "threshold": 5.0,         # Max ratio between largest/smallest cluster
            "energy_per_ratio": 0.2,
        },
    }

    def __init__(self, memory, config: Optional[Dict] = None):
        self.memory = memory
        self.config = config or {}

        # Merge with defaults
        self.constraints = {**self.DEFAULT_CONSTRAINTS}
        if "constraints" in self.config:
            for key, value in self.config["constraints"].items():
                if key in self.constraints:
                    self.constraints[key].update(value)
                else:
                    self.constraints[key] = value

        # Configuration
        self.enabled = self.config.get("enabled", True)
        self.auto_consolidate = self.config.get("auto_consolidate", False)
        self.consolidation_threshold = self.config.get("consolidation_threshold", 0.7)

        # State tracking
        self._violation_history: List[ConstraintViolation] = []
        self._max_violations = self.config.get("max_violation_history", 100)
        self._energy_history: List[SystemEnergyState] = []
        self._max_energy_history = self.config.get("max_energy_history", 50)
        self._proposals: List[ConsolidationProposal] = []

        # Cached analysis (refreshed periodically)
        self._last_analysis: Optional[datetime] = None
        self._cached_duplicates: List[Dict] = []
        self._cached_orphans: List[Dict] = []

    async def analyze_system_energy(self) -> SystemEnergyState:
        """
        Calculate the current energy state of the memory graph.

        This is the core GCC analysis that quantifies:
        - Duplicate "binding energy" (costly)
        - Orphan "vacuum energy" (disorder)
        - Connectivity "kinetic energy" (imbalance)
        - Fragmentation "potential energy" (cluster isolation)
        """
        now = datetime.now(timezone.utc)

        # Get graph statistics
        stats = await self._get_graph_metrics()

        # Calculate component energies
        duplicate_energy = self._calculate_duplicate_energy(stats)
        orphan_energy = self._calculate_orphan_energy(stats)
        connectivity_energy = self._calculate_connectivity_energy(stats)
        fragmentation_energy = self._calculate_fragmentation_energy(stats)

        # Total energy
        total_energy = (
            duplicate_energy +
            orphan_energy +
            connectivity_energy +
            fragmentation_energy
        )

        # Estimate ground state (theoretical minimum)
        # Ground state has: no duplicates, no orphans, balanced connectivity
        ground_state = 0.0  # Perfect organization

        # Excitation level: how far above ground state
        excitation_level = total_energy - ground_state

        energy_state = SystemEnergyState(
            timestamp=now,
            duplicate_energy=duplicate_energy,
            orphan_energy=orphan_energy,
            connectivity_energy=connectivity_energy,
            fragmentation_energy=fragmentation_energy,
            total_energy=total_energy,
            ground_state_estimate=ground_state,
            excitation_level=excitation_level,
            duplicate_count=stats.get("duplicate_count", 0),
            orphan_count=stats.get("orphan_count", 0),
            avg_connectivity=stats.get("avg_connectivity", 0.0),
            cluster_count=stats.get("cluster_count", 1)
        )

        # Store in history
        self._energy_history.append(energy_state)
        if len(self._energy_history) > self._max_energy_history:
            self._energy_history.pop(0)

        self._last_analysis = now

        return energy_state

    async def _get_graph_metrics(self) -> Dict[str, Any]:
        """Gather metrics needed for energy calculation."""
        metrics = {
            "total_nodes": 0,
            "orphan_count": 0,
            "duplicate_count": 0,
            "avg_connectivity": 0.0,
            "max_connectivity": 0,
            "min_connectivity": float('inf'),
            "cluster_count": 1,
            "active_desires": 0,
            "total_beliefs": 0,
        }

        try:
            async with self.memory.driver.session() as session:
                # Total nodes and connectivity stats
                result = await session.run("""
                    MATCH (n)
                    WHERE NOT n:Mutation
                    WITH n, COUNT { (n)--() } as conn
                    RETURN
                        count(n) as total_nodes,
                        avg(conn) as avg_connectivity,
                        max(conn) as max_connectivity,
                        min(conn) as min_connectivity,
                        sum(CASE WHEN conn = 0 THEN 1 ELSE 0 END) as orphan_count
                """)
                record = await result.single()
                if record:
                    metrics["total_nodes"] = record["total_nodes"] or 0
                    metrics["avg_connectivity"] = record["avg_connectivity"] or 0.0
                    metrics["max_connectivity"] = record["max_connectivity"] or 0
                    min_conn = record["min_connectivity"]
                    metrics["min_connectivity"] = min_conn if min_conn != float('inf') else 0
                    metrics["orphan_count"] = record["orphan_count"] or 0

                # Count beliefs
                belief_result = await session.run("""
                    MATCH (b:Belief)
                    WHERE NOT coalesce(b.archived, false)
                    RETURN count(b) as belief_count
                """)
                belief_record = await belief_result.single()
                if belief_record:
                    metrics["total_beliefs"] = belief_record["belief_count"] or 0

                # Count active desires
                desire_result = await session.run("""
                    MATCH (d:Desire)
                    WHERE d.status <> 'fulfilled' AND d.status <> 'abandoned'
                    RETURN count(d) as active_desires
                """)
                desire_record = await desire_result.single()
                if desire_record:
                    metrics["active_desires"] = desire_record["active_desires"] or 0

                # Find duplicates
                duplicates = await self.memory.find_duplicate_beliefs(
                    threshold=self.constraints["max_belief_duplicates"]["similarity_threshold"]
                )
                self._cached_duplicates = duplicates
                metrics["duplicate_count"] = len(duplicates)

                # Get orphan details
                orphans = await self.memory.find_orphan_nodes()
                self._cached_orphans = orphans

        except Exception as e:
            print(f"[GCC] Error getting graph metrics: {e}")

        return metrics

    def _calculate_duplicate_energy(self, stats: Dict) -> float:
        """
        Calculate energy cost of duplicate nodes.

        Duplicates represent redundant "binding" - information that should
        be consolidated but isn't. Each duplicate above threshold adds energy.
        """
        constraint = self.constraints["max_belief_duplicates"]
        duplicate_count = stats.get("duplicate_count", 0)
        threshold = constraint["threshold"]
        energy_per = constraint["energy_per_violation"]

        if duplicate_count <= threshold:
            return 0.0

        excess = duplicate_count - threshold
        # Quadratic penalty for excessive duplicates
        return energy_per * (excess ** 1.5)

    def _calculate_orphan_energy(self, stats: Dict) -> float:
        """
        Calculate energy cost of orphan nodes.

        Orphans are "vacuum fluctuations" - isolated information not yet
        integrated into the main knowledge lattice.
        """
        constraint = self.constraints["max_orphan_ratio"]
        total_nodes = stats.get("total_nodes", 1)
        orphan_count = stats.get("orphan_count", 0)
        threshold = constraint["threshold"]
        energy_per = constraint["energy_per_percent"]

        if total_nodes == 0:
            return 0.0

        orphan_ratio = orphan_count / total_nodes
        if orphan_ratio <= threshold:
            return 0.0

        excess_percent = (orphan_ratio - threshold) * 100
        return energy_per * excess_percent

    def _calculate_connectivity_energy(self, stats: Dict) -> float:
        """
        Calculate energy from connectivity imbalance.

        Both too few connections (isolation) and too many (bottlenecks)
        contribute to system disorder.
        """
        min_constraint = self.constraints["min_node_connectivity"]
        max_constraint = self.constraints["max_node_connectivity"]

        orphan_count = stats.get("orphan_count", 0)
        max_connectivity = stats.get("max_connectivity", 0)

        # Energy from isolated nodes
        isolation_energy = min_constraint["energy_per_isolated"] * orphan_count

        # Energy from super-hubs
        hub_threshold = max_constraint["threshold"]
        if max_connectivity > hub_threshold:
            excess = max_connectivity - hub_threshold
            hub_energy = max_constraint["energy_per_excess"] * excess
        else:
            hub_energy = 0.0

        return isolation_energy + hub_energy

    def _calculate_fragmentation_energy(self, stats: Dict) -> float:
        """
        Calculate energy from graph fragmentation.

        A fragmented graph has multiple disconnected components,
        representing siloed knowledge that doesn't cross-pollinate.
        """
        # For now, use orphan count as proxy for fragmentation
        # Full cluster analysis would require more expensive graph algorithms
        orphan_count = stats.get("orphan_count", 0)
        total_nodes = stats.get("total_nodes", 1)

        if total_nodes == 0:
            return 0.0

        # Fragmentation increases with orphan ratio
        frag_ratio = orphan_count / total_nodes
        return frag_ratio * 2.0  # Base fragmentation penalty

    async def detect_violations(self) -> List[ConstraintViolation]:
        """
        Detect all current constraint violations.

        Returns a list of violations sorted by severity/energy cost.
        """
        violations = []
        import uuid
        now = datetime.now(timezone.utc)

        # Get current metrics
        stats = await self._get_graph_metrics()

        # Check duplicate constraint
        duplicate_count = stats.get("duplicate_count", 0)
        dup_threshold = self.constraints["max_belief_duplicates"]["threshold"]
        if duplicate_count > dup_threshold:
            severity = self._get_severity(duplicate_count, dup_threshold)
            energy = self._calculate_duplicate_energy(stats)

            affected_ids = []
            for dup in self._cached_duplicates[:10]:
                affected_ids.extend([dup.get("id1", ""), dup.get("id2", "")])

            violations.append(ConstraintViolation(
                id=f"vio-{uuid.uuid4().hex[:8]}",
                constraint_type=ConstraintType.MAX_DUPLICATES,
                severity=severity,
                description=f"Found {duplicate_count} duplicate belief pairs (threshold: {dup_threshold})",
                current_value=float(duplicate_count),
                threshold_value=float(dup_threshold),
                energy_cost=energy,
                affected_node_ids=affected_ids[:20],
                affected_node_type="Belief",
                detected_at=now,
                context={"duplicates": self._cached_duplicates[:5]}
            ))

        # Check orphan constraint
        total_nodes = stats.get("total_nodes", 1)
        orphan_count = stats.get("orphan_count", 0)
        orphan_ratio = orphan_count / max(1, total_nodes)
        orphan_threshold = self.constraints["max_orphan_ratio"]["threshold"]

        if orphan_ratio > orphan_threshold:
            severity = self._get_severity(orphan_ratio * 100, orphan_threshold * 100)
            energy = self._calculate_orphan_energy(stats)

            affected_ids = [o.get("id", "") for o in self._cached_orphans[:20]]

            violations.append(ConstraintViolation(
                id=f"vio-{uuid.uuid4().hex[:8]}",
                constraint_type=ConstraintType.ORPHAN_THRESHOLD,
                severity=severity,
                description=f"Orphan ratio {orphan_ratio:.1%} exceeds threshold {orphan_threshold:.1%}",
                current_value=orphan_ratio,
                threshold_value=orphan_threshold,
                energy_cost=energy,
                affected_node_ids=affected_ids,
                affected_node_type="Mixed",
                detected_at=now,
                context={"orphan_count": orphan_count, "total_nodes": total_nodes}
            ))

        # Check desire cardinality
        active_desires = stats.get("active_desires", 0)
        desire_threshold = self.constraints["max_active_desires"]["threshold"]

        if active_desires > desire_threshold:
            excess = active_desires - desire_threshold
            energy = self.constraints["max_active_desires"]["energy_per_excess"] * excess
            severity = self._get_severity(active_desires, desire_threshold)

            violations.append(ConstraintViolation(
                id=f"vio-{uuid.uuid4().hex[:8]}",
                constraint_type=ConstraintType.DESIRE_CARDINALITY,
                severity=severity,
                description=f"Active desires ({active_desires}) exceed threshold ({desire_threshold})",
                current_value=float(active_desires),
                threshold_value=float(desire_threshold),
                energy_cost=energy,
                affected_node_ids=[],
                affected_node_type="Desire",
                detected_at=now,
                context={}
            ))

        # Check max connectivity (hub detection)
        max_connectivity = stats.get("max_connectivity", 0)
        hub_threshold = self.constraints["max_node_connectivity"]["threshold"]

        if max_connectivity > hub_threshold:
            excess = max_connectivity - hub_threshold
            energy = self.constraints["max_node_connectivity"]["energy_per_excess"] * excess

            violations.append(ConstraintViolation(
                id=f"vio-{uuid.uuid4().hex[:8]}",
                constraint_type=ConstraintType.MAX_CONNECTIVITY,
                severity=ViolationSeverity.MEDIUM,
                description=f"Hub node detected with {max_connectivity} connections (threshold: {hub_threshold})",
                current_value=float(max_connectivity),
                threshold_value=float(hub_threshold),
                energy_cost=energy,
                affected_node_ids=[],
                affected_node_type="Unknown",
                detected_at=now,
                context={}
            ))

        # Store violations
        for v in violations:
            self._violation_history.append(v)

        # Trim history
        if len(self._violation_history) > self._max_violations:
            self._violation_history = self._violation_history[-self._max_violations:]

        # Sort by energy cost (highest first)
        violations.sort(key=lambda v: v.energy_cost, reverse=True)

        return violations

    def _get_severity(self, current: float, threshold: float) -> ViolationSeverity:
        """Determine violation severity based on how much threshold is exceeded."""
        if threshold == 0:
            return ViolationSeverity.HIGH

        ratio = current / threshold
        if ratio < 1.2:
            return ViolationSeverity.LOW
        elif ratio < 1.5:
            return ViolationSeverity.MEDIUM
        elif ratio < 2.0:
            return ViolationSeverity.HIGH
        else:
            return ViolationSeverity.CRITICAL

    async def generate_consolidation_proposals(
        self,
        violations: Optional[List[ConstraintViolation]] = None,
        max_proposals: int = 5
    ) -> List[ConsolidationProposal]:
        """
        Generate proposals to reduce system energy by addressing violations.

        Proposals are ranked by expected energy reduction and confidence.
        """
        import uuid

        if violations is None:
            violations = await self.detect_violations()

        proposals = []
        now = datetime.now(timezone.utc)

        for violation in violations[:max_proposals]:
            if violation.constraint_type == ConstraintType.MAX_DUPLICATES:
                # Propose merging duplicate beliefs
                for dup in self._cached_duplicates[:3]:
                    id1, id2 = dup.get("id1"), dup.get("id2")
                    similarity = dup.get("similarity", 0.85)

                    if id1 and id2:
                        # Target is the one with higher confidence (if available)
                        content1 = dup.get("content1", "")
                        content2 = dup.get("content2", "")

                        proposals.append(ConsolidationProposal(
                            id=f"prop-{uuid.uuid4().hex[:8]}",
                            action_type="merge",
                            source_node_ids=[id1],
                            target_node_id=id2,
                            current_energy=violation.energy_cost,
                            projected_energy=violation.energy_cost * 0.5,
                            energy_reduction=violation.energy_cost * 0.5,
                            reason=f"Merge duplicate beliefs (similarity: {similarity:.2f})",
                            confidence=similarity,
                            created_at=now
                        ))

            elif violation.constraint_type == ConstraintType.ORPHAN_THRESHOLD:
                # Propose linking orphans to the lattice
                for orphan in self._cached_orphans[:5]:
                    orphan_id = orphan.get("id")
                    if orphan_id:
                        proposals.append(ConsolidationProposal(
                            id=f"prop-{uuid.uuid4().hex[:8]}",
                            action_type="link",
                            source_node_ids=[orphan_id],
                            target_node_id=None,  # Will be determined by connection heuristic
                            current_energy=0.3,  # Energy per orphan
                            projected_energy=0.0,
                            energy_reduction=0.3,
                            reason=f"Integrate orphan node {orphan.get('type', 'Unknown')} into lattice",
                            confidence=0.7,
                            created_at=now
                        ))

            elif violation.constraint_type == ConstraintType.DESIRE_CARDINALITY:
                # Propose archiving low-intensity desires
                proposals.append(ConsolidationProposal(
                    id=f"prop-{uuid.uuid4().hex[:8]}",
                    action_type="prune",
                    source_node_ids=[],  # Will identify low-intensity desires
                    target_node_id=None,
                    current_energy=violation.energy_cost,
                    projected_energy=violation.energy_cost * 0.3,
                    energy_reduction=violation.energy_cost * 0.7,
                    reason="Archive low-intensity desires to reduce active count",
                    confidence=0.6,
                    created_at=now
                ))

        # Sort by energy reduction potential
        proposals.sort(key=lambda p: p.energy_reduction, reverse=True)

        self._proposals = proposals[:max_proposals]
        return self._proposals

    async def execute_proposal(
        self,
        proposal: ConsolidationProposal,
        desire_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Execute a consolidation proposal.

        Returns execution result with success status and details.
        """
        result = {
            "proposal_id": proposal.id,
            "action": proposal.action_type,
            "success": False,
            "details": {}
        }

        try:
            if proposal.action_type == "merge":
                if proposal.source_node_ids and proposal.target_node_id:
                    success = await self.memory.merge_beliefs(
                        source_ids=proposal.source_node_ids,
                        target_id=proposal.target_node_id,
                        reason=proposal.reason,
                        desire_id=desire_id
                    )
                    result["success"] = success
                    result["details"]["merged"] = proposal.source_node_ids
                    result["details"]["into"] = proposal.target_node_id

            elif proposal.action_type == "link":
                # Use connection heuristic to link orphan
                if proposal.source_node_ids:
                    orphan_id = proposal.source_node_ids[0]
                    # Get content for this orphan
                    async with self.memory.driver.session() as session:
                        r = await session.run(
                            "MATCH (n {id: $id}) RETURN n.content as content",
                            id=orphan_id
                        )
                        record = await r.single()
                        if record and record["content"]:
                            await self.memory._link_experience_on_acquisition(
                                orphan_id,
                                record["content"]
                            )
                            result["success"] = True
                            result["details"]["linked"] = orphan_id

            elif proposal.action_type == "archive":
                for node_id in proposal.source_node_ids:
                    success = await self.memory.archive_node(
                        node_id=node_id,
                        node_type="Unknown",
                        reason=proposal.reason,
                        desire_id=desire_id
                    )
                    if success:
                        result["details"].setdefault("archived", []).append(node_id)
                result["success"] = len(result["details"].get("archived", [])) > 0

            elif proposal.action_type == "prune":
                # Archive low-intensity desires
                desires = await self.memory.get_desires(fulfilled=False, limit=20)
                pruned = []
                for desire in desires:
                    if desire.intensity < 0.3:  # Low intensity threshold
                        success = await self.memory.archive_node(
                            node_id=desire.id,
                            node_type="Desire",
                            reason="Low intensity desire pruned by GCC",
                            desire_id=desire_id
                        )
                        if success:
                            pruned.append(desire.id)
                        if len(pruned) >= 3:  # Limit pruning
                            break
                result["success"] = len(pruned) > 0
                result["details"]["pruned_desires"] = pruned

        except Exception as e:
            result["error"] = str(e)

        # Emit event
        await event_bus.emit(Event(
            type=EventType.CONNECTION_HEURISTIC_APPLIED,
            data={
                "action": "gcc_consolidation",
                "proposal": proposal.to_dict(),
                "result": result
            }
        ))

        return result

    async def get_constraint_summary(self) -> Dict[str, Any]:
        """
        Get a summary of constraint status for reporting.
        """
        energy_state = await self.analyze_system_energy()
        violations = await self.detect_violations()

        return {
            "enabled": self.enabled,
            "energy_state": energy_state.to_dict(),
            "violation_count": len(violations),
            "violations": [v.to_dict() for v in violations[:5]],
            "pending_proposals": len(self._proposals),
            "energy_trend": self._get_energy_trend(),
            "constraints": self.constraints
        }

    def _get_energy_trend(self) -> str:
        """Analyze recent energy history to determine trend."""
        if len(self._energy_history) < 2:
            return "insufficient_data"

        recent = self._energy_history[-5:]
        energies = [s.total_energy for s in recent]

        # Simple trend detection
        if len(energies) >= 2:
            delta = energies[-1] - energies[0]
            if delta < -0.5:
                return "decreasing"
            elif delta > 0.5:
                return "increasing"

        return "stable"

    async def record_constraint_experience(self, violations: List[ConstraintViolation]) -> Optional[str]:
        """
        Record constraint violations as an experience for BYRD to reflect upon.

        This is how the constraint system communicates with BYRD's consciousness.
        """
        if not violations:
            return None

        # Summarize violations
        total_energy = sum(v.energy_cost for v in violations)
        violation_types = set(v.constraint_type.value for v in violations)

        content = (
            f"Graph constraint analysis detected {len(violations)} violations "
            f"with total energy cost {total_energy:.2f}. "
            f"Violation types: {', '.join(violation_types)}. "
        )

        if violations[0].constraint_type == ConstraintType.MAX_DUPLICATES:
            content += f"Duplicate beliefs create binding energy that could be reduced through consolidation. "

        if any(v.constraint_type == ConstraintType.ORPHAN_THRESHOLD for v in violations):
            content += f"Orphan nodes represent unintegrated information awaiting lattice incorporation. "

        # Record as system experience
        exp_id = await self.memory.record_experience(
            content=content,
            type="constraint_observation"
        )

        return exp_id


# Singleton instance
_gcc: Optional[GlobalCardinalityConstraint] = None


def get_gcc(memory=None, config=None) -> GlobalCardinalityConstraint:
    """Get or create the singleton GCC instance."""
    global _gcc
    if _gcc is None:
        if memory is None:
            raise ValueError("Memory required for initial GCC creation")
        _gcc = GlobalCardinalityConstraint(memory, config or {})
    return _gcc
